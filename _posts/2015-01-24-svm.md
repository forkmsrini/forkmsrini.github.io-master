---
layout: post
title: My Understanding of SVM [Research]
---
###Introduction
The task of classification is now applicable to many domains of research. From classifying malignant tumours to words in knowledge bases, classification among classes is almost everywhere.

<!--The Support Vector Machine is once such machine learning algorithm that is used to distinguish between elements of various classes. SVM is a supervised non-probabilistic learning algorithm. The invention of SVM is to the credit to of  Vladimir Naumovich Vapnik and Alexey Yakovlevich Chervonenkis back in 1963. The usable flavour of risk minimisation was proposed by  Vladimir Vapnik and Corinna Cortes 1993 (30 years later!). Statistical learning methods had an Empirical Risk Minimisation (ERM) approach where as SVM are formulated on techniques of Structural Risk Minimisation (SRM) [1]. The ERM attempt to minimise the error in the training data where as SRM minimises an upper bound of the generalisation error.
-->

###Why Support Vector Machines?
The goal is to separate the binary classes by a function which is induced from given data such that it produce a classifier that will work well on unseen data point, i.e. it generalises well. The SV machines algorithm is proved to be a practical technique in statistical learning theory owing to its rich resource of literature.

###The dividing surface (a.k.a Margin)
We can perceive this problem in two-stages, viz.,: learning and labelling. The 'learning' phase deals with the technique of capturing the pattern in the dataset (which shall be referred as the training set henceforth) and the labelling phase is about assigning the correct class label to the new and unseen dataset (which shall be referred as the testing set henceforth).

Now consider a dataset of <span>$$n$$</span> data point with <span>$$d$$</span> attributes. Mathematically <span>$$x_{i} \in \mathbb{R}^d, i=\{1, ..., n\}$$</span>. A linear model can be defined as <span>$$y(x) = w \phi(x) + b$$</span>. Since we consider a binary class problem here, <span>$$y(x)$$</span> takes <span>$$\{\pm1\}$$</span> corresponding to the two classes. <span>$$\phi(x)$$</span> is a mapping function at relates the feature space and the coordinates of the datapoint. <span>$$w^T$$</span> is the feature matrix of the datapoint. <span>$$b$$</span> is a distinct vector from the origin. To classify new datapoint into either of the classes, we are required to learn a model. 

<img style="float:right" src="/public/images/postImages/svm/image1.png" width='480'>

Suppose there exists a hyperplane separating the distributions of the two classes respectively. Geometrically, there can be many possible hyperplanes obtained by changing (perturbation) the values of <span>$$w^T$$</span> and <span>$$b$$</span>.We now have to arrive at the notion of the _best_ method to identify a line, such that, the distance between the hyperplane and each datapoint <span>$$x_i$$</span> is minimum.

Let's paint out an example to understand our current position. From looking at the above image, it is clear that there are two types of points. Now, _assume_ that we have model trained and you're told to choose a specific datapoint within this space. To correctly classify this chosen datapoint, we refer to the fact of which _side_ the datapoint belongs to and then declare a label. Hence, it now safe to say, if we pick the wrong hyperplane from the set of all planes obtained by varying the parameters of the linear model, our prediction will be incorrect. Intuitively the _best_ hyperplane would the mean of all the planes which lies in the centre of both classes. In a general sense, our goal here is to minimise the error of choosing the incorrect hyperplane.

So, anything above the decision boundary should have label <span>$$+1$$</span>: <span>$$x_i$$</span> s.t. <span>$$w^Tx_i + b \gt 0$$</span> shall have <span>$$y_i = 1$$</span>.  Similarly, datapoints below the decision boundary should have label<span>$$-1$$</span>: <span>$$x_i$$</span> s.t. <span>$$w^Tx_i + b \lt 0$$</span> shall have <span>$$y_i = -1$$</span>. 

This decision boundary can be further condensed as $$y(x) = sign(w^Tx_i + b)$$. Hence we can verify if an instance is rightly classified by ensuring  $$y(x) = sign(w^Tx_i + b) \gt 1$$.

There is a space between hyperplane and the first elements of each class. We'll now attempt to classify the elements from these margin such that <span>$$w^Tx_i + b = 1$$</span> belongs to the class with label 1 and <span>$$w^Tx_i + b = -1$$</span> belongs to the class with label -1. 

###Maximising the margin
It is clear that the margins are parallel to the hyperplane. This implies the parameters <span>$$w^T$$</span> and <span>$$b$$</span> apply to the two margins as well. Consider a point <span>$$x_i$$</span> on the <span>$$w^T + b = -1$$</span>. Then a point on <span>$$w^T + b = 1$$</span> to <span>$$x_i$$</span> is given by <span>$$x_2 = x_1 + \lambda w$$</span>. The  closest point
will always lie on the perpendicular to these parallel lines. Vector <span>$$\overrightarrow w$$</span> is perpendicular to both lines.  So the line segment <span>$$\lambda \overrightarrow w$$</span> will be the shortest path connecting points <span>$$x_1$$</span> and <span>$$x_2$$</span> and the distance between the two points will be <span>$$\lambda ||w||$$</span>. 

Solving for  <span>$$\lambda$$</span>: <br/> 
<span>$$ \Rightarrow w^Tx_2 + b = 1$$</span> where <span>$$x_2 = x_1 + \lambda w$$</span> <br/>
<span>$$ \Rightarrow w^T ( x_1 + \lambda w) + b = 1$$</span> \{substituting <span>$$x_2 = x_1 + \lambda w$$</span> \}<br/>
<span>$$ \Rightarrow w^Tx_1 + \lambda w^Tw + b = 1$$</span><br/>
 But, w.k.t. <span>$$w^T x_1 + b = −1$$</span>. We'll now substituting this in the above equation<br/>
<span>$$ \Rightarrow -1 + \lambda w^Tw = 1$$</span><br/>
<span>$$ \Rightarrow \lambda w^Tw = 2 \Rightarrow \lambda = \frac2{w^Tw} = \frac2{||w||^2}$$</span><br/>

Now putting the value of <span>$$\lambda$$</span> back in <span>$$\lambda||w||$$</span>, we have: <br/>
<span>$$\frac2{||w||^2} ||w|| \Rightarrow \frac2{||w||}\Rightarrow \frac2{\sqrt{ww^T}}$$</span>

Clearly, we will try to maximise this distance so that the misclassification error is reduced and thus the datapoints from each class are at the maximum distance. This is the 'actual margin.' Thus we must minimise a monotonic function  <span>$$ \frac{ww^T}2 $$</span>. <span>$$ \left( \frac2{\sqrt{ww^T}} \Rightarrow \frac{\sqrt{ww^T}}2 \Rightarrow \frac{ww^T}2 \right) $$</span>

###Soft and hard margins
Suppose in our previous discussions the datapoint were not uniformly distributed, a few were astray, the method of classification would fail to perform due to these outliers. In order to account for this practical behaviour of datapoints we introduce _slack variables,_ <span>$$\varepsilon_i \ge 0; \forall x_i$$</span>. The reason for introducing a slack variable is to control the degree of flexibility by means of penalties. <br/>
$$x_i =
\begin{cases}
on The Correct Side, But Within Or On The Margin Boundary;  & \text{p = [0,1)} \\
on The Hyperplane; & \text{p = 1}\\
on The Wrong Side Of The Hyperplane; & \text{p} \gt \text{1}
\end{cases}$$

We can conveniently formalise this into a quadratic programming problem as below:<br/>
$$min_{w,b} \frac{ww^T}2 s.t.: y_i(w^T x_i + b) \gt 1 (\forall x_i)$$

And with the penalties:<br/>
$$min_{w,b,\varepsilon_i} \frac{ww^T}2 + \mathbb{C}\sum_i \varepsilon_i s.t.: y_i(w^T x_i + b) \ge 1-\varepsilon_i \text{ }\& \text{ } \varepsilon_i \gt 0 (\forall x_i)$$

###Problem of  linear separability
Many times, mapping <span>$$\overrightarrow w$$</span> to a higher dimensional spaces (upto infinity, if required) make them  linearly separable whereas they may not  linearly separable in their original vector spaces. Consider the mapping <span>$$x_i \rightarrow \phi(x_i)$$</span>, then the above QP can be re-written as:<br/>
$$min_{w,b,\varepsilon_i} \frac{ww^T}2 + \mathbb{C}\sum_i \varepsilon_i s.t.: y_i(w^T \phi(x_i) + b) \ge 1-\varepsilon_i \text{ }\& \text{ } \varepsilon_i \gt 0 (\forall x_i)$$.

###Lagrangian and Dual  Reformulation
The technique of Lagrange multipliers captures the idea that in a particular surface in which we need to search for an optimal value, the gradient directions of both the objective and constraint surface must be oriented oppositely. Further to this, <br/>
$$max_{\alpha_i \ge 0} \text{ } \alpha \left[ 1 − y_i(w^T \phi(x_i) + b)\right]$$.

This ensures that when <span>$$y_i(w^T \phi(x_i) + b) \ge 1$$</span>  the expression above is maximal when <span>$$\alpha_i = 0$$</span> as <span>$$[1 − y_i(w^T \phi(x_i) + b)]$$</span> is always negative. Otherwise, <span>$$y_i(w^T \phi(x_i) +b) < 1$$</span>, so <span>$$[1−y_i(w^T \phi(x_i) + b)]$$</span> is a positive value, and this expression is maximal when <span>$$\alpha_i \rightarrow \infty$$</span>. This has the effect of penalising any misclassified
datapoints, while assigning 0 penalty to properly classified instances. 

This can be captured in:<br/>
$$min_{w,b}\left[\frac{w^Tw}2 + \sum_i max_{\alpha_i \ge 0}\alpha_i[1 − y_i(w^T \phi(x_i) + b)]\right]$$
We now impose a constraint on the Lagrange multipliers<span>$$\alpha$$</span> such that <span>$$\alpha \not\rightarrow \infty$$</span> and lie within <span>$$0 \le \alpha_i \le \mathbb{C}$$</span>. This is done to accommodate the slack variables.

A dual problem can be defined as below from the above conditions (that <span>$$0 \le \alpha_i \le \mathbb{C}$$</span>):<br/>
$$max_{\alpha \ge 0}[min_{w,b} \mathcal{J}(w, b; α)] $$
where
$$ \mathcal{J}\{w,b;\alpha\} = \frac{w^Tw}2 + \sum_i \alpha_i [1 - y_i(w^T \phi(x_i) + b)]$$

Clearly, we recognise this as an optimisation problem. By setting <span>$$\frac{\partial\mathcal{J}}{\partial w} = 0 $$</span> and <span>$$\frac{\partial\mathcal{J}}{\partial b} = 0 $$</span> we find the optimal setting of <span>$$w$$</span> as <span>$$\sum_i \alpha_i y_i \phi(x_i)$$</span> and <span>$$b$$</span> yields a constraint <span>$$\sum_i \alpha_i y_i = 0$$</span> respectively.

Performing a series of mathematical reductions and substitutions, we arrive at:<br/>
$$min_{w,b}\mathcal{J}(w, b; \alpha) = \sum_i \alpha_i − \frac12 \sum_{i,j} \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)$$

Thus the _dual form_ can be written as:<br/>
$$max_{\alpha \ge 0}\left[\sum_i \alpha_i −\frac12 \sum{i,j}\alpha_i \alpha_j y_i y_j \phi(xi)^T \phi(x_j)\right] s.t.  \sum_i \alpha_i y_i = 0 \text{ and } 0 \le \alpha_i \le \mathbb{C}$$

###Kernel trick (having very large data points)
Assume in order to obtain correct classification results, we had to map the dataset to infinite number of feature vectors, calculating <span>$$\phi(x_i)^T \phi(x_j)$$</span> may be intractable. In such situations, the dual enables this as we need to only specify the kernel matrix <span>$$K$$</span> containing entries that represent the feature space mapping of 2 features vectors representing 2 datapoints from the dataset. It turns out that the matrix <span>$$K$$</span>  that operate on the lower dimension vectors <span>$$x_i$$</span> and <span>$$x_j$$</span> to produce a value equivalent to the dot product
of the higher-dimensional vectors. Looking at the original space containing the untransformed dataset, we have constructed a complex non-linear decision boundary. But in order to achieve this, we have to calculate the kernel matrix, <span>$$K$$</span>. This will lead to a computational overhead provided we use a kernel matrix. 

Now, introducing the kernel trick i the dual form, we have:<br/>
$$max_{\alpha \ge 0}\left[\sum_i \alpha_i −\frac12 \sum_{i,j}\alpha_i \alpha_j y_i y_j K(x_i, x_j)\right]$$

###Support Vectors
Once we have learned the optimal parameters <span>$$\alpha_i$$</span> our task is rather simple. We calculate<br/>
$$y(x) = sign(w^Tx + b) = \sum_i \alpha_i y_i K(x_i, x) + b$$
where <span>$$x$$</span> is our instance of a datapoint. <span>$$\alpha_i$$ is non-zero only for instances <span>$$\phi(x_i)$$</span> that lie on or near the  decision boundary. These instances are called _Support Vector_. 


###Conclusion
If you have scrolled right down here. You have done just right. Look further down and the references may help you understand much better than my attempt. 


###References:
[1] V. Vapnik. _The Nature of Statistical Learning Theory_. Springer-Verlag, New York, 1995.

[2] CJC Burges. _A Tutorial on Support Vector Machines for Pattern
Recognition_. <a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">(PDF)</a>

[3] Nello Cristianini. _Support Vector and Kernel Machines_. <a href="http://www.support-vector.net/icml-tutorial.pdf">(PDF)</a> ICML Tutorial.

[4] Jason Weston. _Support Vector Machine Tutorial_. <a href="http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf">(PDF)</a>




